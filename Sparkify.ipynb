{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Librariesimport os\n",
    "import re\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf, count, when, isnull, collect_list\n",
    "from pyspark.sql.types import IntegerType, BooleanType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import importlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in mini sparkify dataset in the local environment\n",
    "event_data = \"./mini_sparkify_event_data.json\"\n",
    "df = spark.read.json(event_data)\n",
    "\n",
    "# Let's see the preview of the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286500, 18)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['artist',\n",
       " 'auth',\n",
       " 'firstName',\n",
       " 'gender',\n",
       " 'itemInSession',\n",
       " 'lastName',\n",
       " 'length',\n",
       " 'level',\n",
       " 'location',\n",
       " 'method',\n",
       " 'page',\n",
       " 'registration',\n",
       " 'sessionId',\n",
       " 'song',\n",
       " 'status',\n",
       " 'ts',\n",
       " 'userAgent',\n",
       " 'userId']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "| Submit Registration|\n",
      "|            Settings|\n",
      "|               Login|\n",
      "|            Register|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see what all pages have been visited by users\n",
    "df.select('page').distinct().show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns\n",
    "df_clean = df.select('artist','auth','firstName','gender','lastName','length','level','location','page','song','ts','userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all auths in one column so that we can search for Cancelled i.e. our churn definition\n",
    "df_churn = df_clean.groupby('userId').agg(collect_list('auth').alias(\"auths\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|userId|               auths|\n",
      "+------+--------------------+\n",
      "|100010|[Logged In, Logge...|\n",
      "+------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_churn.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom UDF for creating the churned column\n",
    "udf_churned = udf(lambda x: 'Cancelled' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the churned column and drop the auths column\n",
    "df_churn = df_churn.withColumn(\"Churned\", udf_churned(df_churn.auths))\n",
    "df_churn = df_churn.drop('auths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the churn df with the clean df on the basis of user ID\n",
    "df_label = df_churn.join(df_clean,'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+---------+---------+------+---------+---------+-----+--------------------+--------+--------------------+-------------+\n",
      "|userId|Churned|              artist|     auth|firstName|gender| lastName|   length|level|            location|    page|                song|           ts|\n",
      "+------+-------+--------------------+---------+---------+------+---------+---------+-----+--------------------+--------+--------------------+-------------+\n",
      "|100010|  false|Sleeping With Sirens|Logged In| Darianna|     F|Carpenter|202.97098| free|Bridgeport-Stamfo...|NextSong|Captain Tyin Knot...|1539003534000|\n",
      "+------+-------+--------------------+---------+---------+------+---------+---------+-----+--------------------+--------+--------------------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the first record of df_label\n",
    "df_label.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Churned='false', count=174), Row(Churned='true', count=52)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see the distribution of churned column\n",
    "df_label.select([\"userId\",\"Churned\"]).distinct().groupBy(\"Churned\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----+---------+------+--------+------+-----+--------+----+-----+---+\n",
      "|userId|Churned|artist|auth|firstName|gender|lastName|length|level|location|page| song| ts|\n",
      "+------+-------+------+----+---------+------+--------+------+-----+--------+----+-----+---+\n",
      "|     0|      0| 58392|   0|     8346|  8346|    8346| 58392|    0|    8346|   0|58392|  0|\n",
      "+------+-------+------+----+---------+------+--------+------+-----+--------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many null values are there in each column\n",
    "df_label.select([count(when(isnull(column), column)).alias(column) for column in df_label.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: string, Churned: string, artist: string, auth: string, firstName: string, gender: string, lastName: string, length: double, level: string, location: string, page: string, song: string, ts: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Persist the dataframe both in memory and on disk\n",
    "df_label.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Thumbs Up: Filter ‘Thumbs Up’ page visits groups then group that data by users,\n",
    "# aggregates the counts of the ‘Thumbs Up’ page per user, give alias name to the column and finally order the results by userId.\n",
    "thumbs_up = df_label.where(df_label.page=='Thumbs Up').groupby(\"userId\").agg(count(col('page')).alias('ThumbsUp')).orderBy('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Thumbs Down: Filter ‘Thumbs Down’ page visits groups then group that data by users,\n",
    "# aggregates the counts of the ‘Thumbs Down’ page per user, give alias name to the column and finally order the results by userId.\n",
    "thumbs_down = df_label.where(df_label.page=='Thumbs Down').groupby(\"userId\").agg(count(col('page')).alias('ThumbsDown')).orderBy('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join both thumbs up and thumbs join using userID\n",
    "thumbs_up_and_down = thumbs_up.join(thumbs_down,'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Songs Played by a user\n",
    "songs_played = df_label.where(col('song')!='null').groupby(\"userId\").agg(count(col('song')).alias('SongsPlayed')).orderBy('userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join songs played and all thumbs data to the df features\n",
    "df_features = df_churn.join(songs_played,'userId')\n",
    "df_features = df_features.join(thumbs_up_and_down,'userId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number of days user has been using the service\n",
    "days = df_label.groupby('userId').agg(((max(col('ts')) - min(col('ts')))/86400000).alias(\"Days\"))\n",
    "df_features = df_features.join(days, \"userId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+----------+----+\n",
      "|userId|SongsPlayed|ThumbsUp|ThumbsDown|Days|\n",
      "+------+-----------+--------+----------+----+\n",
      "|     0|          0|       0|         0|   0|\n",
      "+------+-----------+--------+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check null count in key features. After running this cell it's clear that it's all zero null in these features.\n",
    "df_features.select([count(when(isnull(column), column)).alias(column) for column in [\"userId\", \"SongsPlayed\", \"ThumbsUp\", \"ThumbsDown\", \"Days\"]]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to avoid the problem which can occur in full data set i.e. \n",
    "# a Spark UDF will return a column of NULLs if the input data type doesn’t match the output data type.\n",
    "udf_thumbs_up_per_song = udf(lambda thumbsUp, songsPlayed: float(thumbsUp)/float(songsPlayed), FloatType())\n",
    "udf_thumbs_down_per_song = udf(lambda thumbsDown, songsPlayed: float(thumbsDown)/float(songsPlayed), FloatType())\n",
    "udf_songs_played_per_hour = udf(lambda songsPlayed, numberOfDays: float(songsPlayed)/float((numberOfDays*24)), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Thumbs UpPerSong using UDF\n",
    "df_features = df_features.withColumn(\"ThumbsUpPerSong\", udf_thumbs_up_per_song(df_features.ThumbsUp, df_features.SongsPlayed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Thumbs DownPerSong using UDF\n",
    "df_features = df_features.withColumn(\"ThumbsDownPerSong\", udf_thumbs_down_per_song(df_features.ThumbsDown, df_features.SongsPlayed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the SongsPerHour feature using UDF\n",
    "df_features = df_features.withColumn(\"SongsPerHour\", udf_songs_played_per_hour(df_features.SongsPlayed, df_features.Days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|Churned| avg(SongsPerHour)|\n",
      "+-------+------------------+\n",
      "|  false|1.3289544926175187|\n",
      "|   true| 2.415751484163264|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see what's the average of SongsPerHour for both churned as well as non churned users\n",
    "# It seems that user who got churned have played more songs compared to users who didn't get churned. \n",
    "# Do they get bored?? Or they didn't like the service after experimenting a lot?\n",
    "df_features.select(\"SongsPerHour\", \"Churned\").groupby(\"Churned\").agg(avg(col('SongsPerHour'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-----------------+----+------------+\n",
      "|SongsPlayed|ThumbsUpPerSong|ThumbsDownPerSong|Days|SongsPerHour|\n",
      "+-----------+---------------+-----------------+----+------------+\n",
      "|          0|              0|                0|   0|           0|\n",
      "+-----------+---------------+-----------------+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.select([count(when(isnull(c), c)).alias(c) for c in [\"SongsPlayed\", \"ThumbsUpPerSong\", \"ThumbsDownPerSong\", \"Days\", \"SongsPerHour\"]]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"SongsPlayed\", \"ThumbsUpPerSong\", \"ThumbsDownPerSong\", \"Days\", \"SongsPerHour\"], outputCol=\"FeatureVector\")\n",
    "df_features = assembler.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"FeatureVector\", outputCol=\"ScaledFeatures\", withStd=True)\n",
    "scaler_tranformer = scaler.fit(df_features)\n",
    "df_features = scaler_tranformer.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+--------+----------+------------------+---------------+-----------------+------------+--------------------+--------------------+\n",
      "|userId|Churned|SongsPlayed|ThumbsUp|ThumbsDown|              Days|ThumbsUpPerSong|ThumbsDownPerSong|SongsPerHour|       FeatureVector|      ScaledFeatures|\n",
      "+------+-------+-----------+--------+----------+------------------+---------------+-----------------+------------+--------------------+--------------------+\n",
      "|100010|  false|        275|      17|         5| 44.21780092592593|    0.061818182|      0.018181818|  0.25913393|[275.0,0.06181818...|[0.24639992057147...|\n",
      "|200002|  false|        387|      21|         6|45.496805555555554|    0.054263566|      0.015503876|  0.35442048|[387.0,0.05426356...|[0.34675188822240...|\n",
      "|   124|  false|       4079|     171|        41|59.996944444444445|     0.04192204|      0.010051483|   2.8327832|[4079.0,0.0419220...|[3.65478282185840...|\n",
      "|    51|   true|       2111|     100|        21|15.779398148148148|    0.047370914|      0.009947892|   5.5742517|[2111.0,0.0473709...|[1.89145539027778...|\n",
      "|     7|  false|        150|       7|         1|50.784050925925925|    0.046666667|      0.006666667| 0.123070136|[150.0,0.04666666...|[0.13439995667535...|\n",
      "|    15|  false|       1914|      81|        14| 54.77318287037037|     0.04231975|     0.0073145246|   1.4560045|[1914.0,0.0423197...|[1.71494344717749...|\n",
      "|    54|   true|       2841|     163|        29| 42.79719907407407|    0.057374164|      0.010207674|    2.765952|[2841.0,0.0573741...|[2.54553517943116...|\n",
      "|   155|  false|        820|      58|         3| 25.82783564814815|     0.07073171|     0.0036585366|   1.3228621|[820.0,0.07073170...|[0.73471976315859...|\n",
      "|100014|   true|        257|      17|         3|41.244363425925926|     0.06614786|     0.0116731515|  0.25963143|[257.0,0.06614785...|[0.23027192577043...|\n",
      "|   132|  false|       1928|      96|        17| 50.49740740740741|    0.049792532|      0.008817428|   1.5908407|[1928.0,0.0497925...|[1.72748744313385...|\n",
      "|   101|   true|       1797|      86|        16|15.861481481481482|     0.04785754|      0.008903729|   4.7205553|[1797.0,0.0478575...|[1.61011148097071...|\n",
      "|    11|  false|        647|      40|         9|53.241585648148146|    0.061823804|      0.013910355|   0.5063398|[647.0,0.06182380...|[0.57971181312635...|\n",
      "|   138|  false|       2070|      95|        24| 56.07674768518518|     0.04589372|      0.011594203|   1.5380707|[2070.0,0.0458937...|[1.85471940211985...|\n",
      "|300017|  false|       3632|     303|        28| 59.11390046296296|     0.08342511|     0.0077092513|   2.5600295|[3632.0,0.0834251...|[3.25427095096585...|\n",
      "|100021|   true|        230|      11|         5|45.457256944444445|    0.047826085|       0.02173913|  0.21082076|[230.0,0.04782608...|[0.20607993356887...|\n",
      "|    29|   true|       3028|     154|        22| 43.32092592592593|    0.050858654|     0.0072655217|   2.9123724|[3028.0,0.0508586...|[2.71308712541977...|\n",
      "|    69|  false|       1125|      72|         9| 50.98648148148148|          0.064|            0.008|  0.91936135|[1125.0,0.0640000...|[1.00799967506513...|\n",
      "|   112|  false|        215|       9|         3| 56.87869212962963|    0.041860465|      0.013953488|  0.15749893|[215.0,0.04186046...|[0.19263993790133...|\n",
      "|    42|  false|       3573|     166|        25| 60.08825231481482|     0.04645956|     0.0069969213|   2.4776058|[3573.0,0.0464595...|[3.20140696800688...|\n",
      "|    73|   true|        377|      14|         7| 21.52954861111111|    0.037135277|      0.018567638|   0.7296174|[377.0,0.03713527...|[0.33779189111071...|\n",
      "+------+-------+-----------+--------+----------+------------------+---------------+-----------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for convert label or targeting variable to integer\n",
    "convertToInt = udf(lambda x: 1 if x==\"true\" else 0, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now copy the churned column to label column post converting to integer\n",
    "df_features = df_features.withColumn('label', convertToInt(df_features.Churned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+--------+----------+------------------+---------------+-----------------+------------+--------------------+--------------------+-----+\n",
      "|userId|Churned|SongsPlayed|ThumbsUp|ThumbsDown|              Days|ThumbsUpPerSong|ThumbsDownPerSong|SongsPerHour|       FeatureVector|      ScaledFeatures|label|\n",
      "+------+-------+-----------+--------+----------+------------------+---------------+-----------------+------------+--------------------+--------------------+-----+\n",
      "|100010|  false|        275|      17|         5| 44.21780092592593|    0.061818182|      0.018181818|  0.25913393|[275.0,0.06181818...|[0.24639992057147...|    0|\n",
      "|200002|  false|        387|      21|         6|45.496805555555554|    0.054263566|      0.015503876|  0.35442048|[387.0,0.05426356...|[0.34675188822240...|    0|\n",
      "|   124|  false|       4079|     171|        41|59.996944444444445|     0.04192204|      0.010051483|   2.8327832|[4079.0,0.0419220...|[3.65478282185840...|    0|\n",
      "|    51|   true|       2111|     100|        21|15.779398148148148|    0.047370914|      0.009947892|   5.5742517|[2111.0,0.0473709...|[1.89145539027778...|    1|\n",
      "|     7|  false|        150|       7|         1|50.784050925925925|    0.046666667|      0.006666667| 0.123070136|[150.0,0.04666666...|[0.13439995667535...|    0|\n",
      "|    15|  false|       1914|      81|        14| 54.77318287037037|     0.04231975|     0.0073145246|   1.4560045|[1914.0,0.0423197...|[1.71494344717749...|    0|\n",
      "|    54|   true|       2841|     163|        29| 42.79719907407407|    0.057374164|      0.010207674|    2.765952|[2841.0,0.0573741...|[2.54553517943116...|    1|\n",
      "|   155|  false|        820|      58|         3| 25.82783564814815|     0.07073171|     0.0036585366|   1.3228621|[820.0,0.07073170...|[0.73471976315859...|    0|\n",
      "|100014|   true|        257|      17|         3|41.244363425925926|     0.06614786|     0.0116731515|  0.25963143|[257.0,0.06614785...|[0.23027192577043...|    1|\n",
      "|   132|  false|       1928|      96|        17| 50.49740740740741|    0.049792532|      0.008817428|   1.5908407|[1928.0,0.0497925...|[1.72748744313385...|    0|\n",
      "|   101|   true|       1797|      86|        16|15.861481481481482|     0.04785754|      0.008903729|   4.7205553|[1797.0,0.0478575...|[1.61011148097071...|    1|\n",
      "|    11|  false|        647|      40|         9|53.241585648148146|    0.061823804|      0.013910355|   0.5063398|[647.0,0.06182380...|[0.57971181312635...|    0|\n",
      "|   138|  false|       2070|      95|        24| 56.07674768518518|     0.04589372|      0.011594203|   1.5380707|[2070.0,0.0458937...|[1.85471940211985...|    0|\n",
      "|300017|  false|       3632|     303|        28| 59.11390046296296|     0.08342511|     0.0077092513|   2.5600295|[3632.0,0.0834251...|[3.25427095096585...|    0|\n",
      "|100021|   true|        230|      11|         5|45.457256944444445|    0.047826085|       0.02173913|  0.21082076|[230.0,0.04782608...|[0.20607993356887...|    1|\n",
      "|    29|   true|       3028|     154|        22| 43.32092592592593|    0.050858654|     0.0072655217|   2.9123724|[3028.0,0.0508586...|[2.71308712541977...|    1|\n",
      "|    69|  false|       1125|      72|         9| 50.98648148148148|          0.064|            0.008|  0.91936135|[1125.0,0.0640000...|[1.00799967506513...|    0|\n",
      "|   112|  false|        215|       9|         3| 56.87869212962963|    0.041860465|      0.013953488|  0.15749893|[215.0,0.04186046...|[0.19263993790133...|    0|\n",
      "|    42|  false|       3573|     166|        25| 60.08825231481482|     0.04645956|     0.0069969213|   2.4776058|[3573.0,0.0464595...|[3.20140696800688...|    0|\n",
      "|    73|   true|        377|      14|         7| 21.52954861111111|    0.037135277|      0.018567638|   0.7296174|[377.0,0.03713527...|[0.33779189111071...|    1|\n",
      "+------+-------+-----------+--------+----------+------------------+---------------+-----------------+------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, test and validation\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "train, test = df_features.randomSplit([train_ratio, test_ratio], seed=9999)\n",
    "train, validation = train.randomSplit([(1 - validation_ratio), validation_ratio], seed=9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for internal so that we can play with multiple models\n",
    "relevant_module_class = 'pyspark.ml.classification'\n",
    "relevant_model_class = 'RandomForestClassifier' # LogisticRegression\n",
    "model_params = {'featuresCol': 'FeatureVector', 'labelCol': 'label', 'maxIter': 10} if relevant_model_class == 'LogisticRegression' else {'featuresCol': 'FeatureVector', 'labelCol': 'label', 'numTrees': 10}\n",
    "relevant_module = importlib.import_module(relevant_module_class)\n",
    "relevant_model = getattr(relevant_module,relevant_model_class)\n",
    "model = relevant_model(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(trained_model,train,validation,test,evaluator):    \n",
    "    # Test the performance via evaluator on training data\n",
    "    predictions = trained_model.transform(train)\n",
    "    print('Train: Area Under ROC', evaluator.setMetricName(\"areaUnderROC\").evaluate(predictions))\n",
    "    print('Train: Area Under PR', evaluator.setMetricName(\"areaUnderPR\").evaluate(predictions))\n",
    "    \n",
    "    # Test the performance via evaluator on validation data\n",
    "    predictions = trained_model.transform(validation)\n",
    "    print('Validation: Area Under ROC', evaluator.setMetricName(\"areaUnderROC\").evaluate(predictions))\n",
    "    print('Validation: Area Under PR', evaluator.setMetricName(\"areaUnderPR\").evaluate(predictions))\n",
    "    \n",
    "    # Test the performance via evaluator on test data\n",
    "    predictions = trained_model.transform(test)\n",
    "    print('Test: Area Under ROC', evaluator.setMetricName(\"areaUnderROC\").evaluate(predictions))\n",
    "    print('Test: Area Under PR', evaluator.setMetricName(\"areaUnderPR\").evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "model = RandomForestClassifier(featuresCol = 'FeatureVector', labelCol = 'label', numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Area Under ROC 0.9985119047619048\n",
      "Train: Area Under PR 0.9942257534428728\n",
      "Validation: Area Under ROC 0.9523809523809523\n",
      "Validation: Area Under PR 0.9027777777777777\n",
      "Test: Area Under ROC 0.8428030303030303\n",
      "Test: Area Under PR 0.7930118994150303\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(trained_model,train,validation,test,evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run through a cross validator\n",
    "param_grid = ParamGridBuilder().addGrid(model.regParam, [0.1, 0.01]).build() if type(model).__name__ == 'LogisticRegression' else ParamGridBuilder().addGrid(model.numTrees, [25, 50, 75, 100]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=model,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "trained_model = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Area Under ROC 0.9985119047619048\n",
      "Train: Area Under PR 0.9942257534428728\n",
      "Validation: Area Under ROC 0.9523809523809523\n",
      "Validation: Area Under PR 0.9027777777777777\n",
      "Test: Area Under ROC 0.8428030303030303\n",
      "Test: Area Under PR 0.7930118994150303\n"
     ]
    }
   ],
   "source": [
    "evaluate_performance(trained_model,train,validation,test,evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CrossValidatorModel' object has no attribute 'featureImportances'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-577410108966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoefficients\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrelevant_model_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LogisticRegression'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatureImportances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'CrossValidatorModel' object has no attribute 'featureImportances'"
     ]
    }
   ],
   "source": [
    "trained_model.coefficients if relevant_model_class == 'LogisticRegression' else trained_model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPERIMENTED BUT DIDN'T USE\n",
    "#training_summary = trained_model.summary\n",
    "\n",
    "# Get the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "# training_summary.roc.show()\n",
    "#print(\"areaUnderROC: \" + str(training_summary.areaUnderROC))\n",
    "\n",
    "# objectiveHistory = training_summary.objectiveHistory\n",
    "# print(\"objectiveHistory:\")\n",
    "# for objective in objectiveHistory:\n",
    "#     print(objective)\n",
    "\n",
    "#f_measure = training_summary.fMeasureByThreshold\n",
    "#max_f_measure = f_measure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "#best_threshold = f_measure.where(f_measure['F-Measure'] == max_f_measure['max(F-Measure)']) \\\n",
    "#    .select('threshold').head()['threshold']\n",
    "\n",
    "#f_measure.show()\n",
    "#print(best_threshold)\n",
    "#print(max_f_measure)\n",
    "\n",
    "# model.setThreshold(best_threshold)\n",
    "\n",
    "#pr = training_summary.pr\n",
    "#pr.show()\n",
    "\n",
    "#predictions.show()\n",
    "#print(predictions.filter(predictions.label == predictions.prediction).count())\n",
    "#print(predictions.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the coefficients, the features that contribute the most are:\n",
    "\n",
    "Average number of thumbsdown per song played\n",
    "Number of songs played"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
